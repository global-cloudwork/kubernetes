**Google Cloud**: The deployment uses several key Google Cloud services. The Google Cloud CLI (`gcloud`) is used locally to create a compute instance, which is then assigned a static external IP so DNS never needs to change. This instance acts as the main entry point for web traffic. Google Secrets stores critical keys: a WireGuard key from the on-site cluster, a key for DNS challenges, and an environment variables file that’s exported into the startup script. Two service accounts are used: one launches the compute instance and has access only to the required secrets, while a secondary account handles DNS challenges via the cluster issuer with the necessary permissions.

**Deployment Script / Cluster Configuration**: The startup script performs the following actions: it exports an environment variables file and instance metadata for IPs, updates system packages via `apt`, and installs WireGuard, K9S, Helm3, and RKE2. Directories for RKE2 and server manifests are created, and the RKE2 configuration is pulled from a mono repo. Environment variables from Google Secrets—including CNI-related settings—are substituted into the configuration file before it’s written. The script enables and starts the RKE2 service, links `kubectl` to a user-local bin, creates `.kube` directories, copies and adjusts ownership of the kubeconfig, and flattens all kubeconfigs. Afterward, it deploys gateway manifests, TLS routes CRDs, and Argo project manifests, waiting for CRDs to be ready.

**Applications / Argo CD Deployment**: All applications that run on the cluster are defined in an `applications` folder. Each application has its own subdirectory named after it, containing YAML files and any customized files needed for deployment. For example, Argo CD has an `applications/ArgoCD` folder with a customization file that sets up the namespace, the HTTP route for the gateway, and specifies a Helm chart. The Helm chart includes the version and overrides values inline rather than using a traditional values file. Applications are deployed using a method in the tenant section where an ApplicationSet combines SCM providers with a cluster generator. This matrix ensures each application is deployed to the correct cluster—development, production, or testing. Each environment receives the core components plus the core, data, edge, and tenant directories for additional features. Cilium is installed like any other application, using this same deployment system.

**Tenant**: Tenant is essentially an ApplicationSet that uses a matrix generator to combine two generators. The first is a cluster generator that outputs the three clusters—development, production, and testing. The second is an SCM provider generator that uses file discovery to find `customization.yaml` files. These files define which applications are deployed to which clusters. If a file indicates an application should be deployed to a specific cluster, the ApplicationSet points to the corresponding customization directory and deploys it there. For example, Vault from HashiCorp can be deployed to the development cluster this way.

**Edge**: The Edge section handles the gateway and the cluster issuer. The gateway listens on the instance’s static external IP for web traffic on the main ports. It’s annotated so that Cert Manager, running on the cluster, can automatically create and manage certificates for the gateway’s HTTPS routes. TLS connections terminate at the gateway, and certificate references are automatically handled by the annotation. The cluster issuer uses an ACME DNS-01 challenge with Google Cloud DNS. The service account allows automated DNS updates. The Google Cloud project name and a secret from Google Secrets are used to create a `key.json` at runtime, letting the cluster issuer generate certificates for all HTTP routes defined in the application directories. The gateway also has its own namespace.